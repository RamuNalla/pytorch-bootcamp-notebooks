{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43a1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5931038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x: tensor([3., 7.])\n",
      "Gradient of y: tensor([ 8., 19.])\n",
      "Gradient of z: tensor([1., 1.])\n",
      "Result of the operation: z = tensor([15., 84.])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with requires_grad=True\n",
    "x = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([3.0, 7.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "z = x * y + y**2\n",
    "\n",
    "z.retain_grad() #By default intermediate layer weight updation is not shown. This way we ask to store the gradient w.r.t z also\n",
    "\n",
    "# Compute the gradients\n",
    "z_sum = z.sum().backward()      # since z is a 2D tensor, .backward() need a scalar. Therefore sum operation before backward\n",
    "\n",
    "\n",
    "print(f\"Gradient of x: {x.grad}\")\n",
    "print(f\"Gradient of y: {y.grad}\")\n",
    "print(f\"Gradient of z: {z.grad}\")\n",
    "print(f\"Result of the operation: z = {z.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddd1a7",
   "metadata": {},
   "source": [
    "The detach() method is used to create a new tensor that shares storage with the original tensor but without tracking operations. When you call detach(), it returns a new tensor that does not require gradients. This is useful when you want to perform operations on a tensor without affecting the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29780718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before detaching z from computation:  True\n",
      "After detaching z from computation:  False\n"
     ]
    }
   ],
   "source": [
    "# Let's detach z from the computation graph\n",
    "print(\"Before detaching z from computation: \", z.requires_grad)\n",
    "z_det = z.detach()\n",
    "print(\"After detaching z from computation: \", z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78297e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
